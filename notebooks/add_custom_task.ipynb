{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e349acc-9bc3-4ade-b427-e00903c63d65",
   "metadata": {},
   "source": [
    "## Loading a new rating task is easy! Just follow these steps:\n",
    "\n",
    "### 1. Create a new config object in the TASK_CONFIGS dictionary of `configs/tasks.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64ff0fa2-f42d-49cb-a870-606893cb9fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate all fields as described below:\n",
    "TASK_CONFIGS = {}\n",
    "TASK_CONFIGS['my_task'] = {\n",
    "    'path': 'TopicalChat/processed_ratings.csv',      # Path to your dataset (including system inputs/outputs and human ratings)\n",
    "    'property': 'Understandable',                      # Name of the property being rated\n",
    "    'n_options': 2,                                    # Number of forced choice options\n",
    "    'n_response_sets': 3,                              # Number of response sets (should be 2^n_options - 1)\n",
    "    'positive_categorization_options': [0],            # Indices of \"positive\" options\n",
    "                                                        # For Yes/No: [0] means 'Yes' is positive\n",
    "                                                        # For A/B/C: [0,1] means A and B are positive, C is negative\n",
    "                                                        # Used for downstream metrics like bias and consistency\n",
    "    'prompt_fields': ['context', 'fact', 'response'],  # Fields used to populate the rating prompt from the CSV\n",
    "    'valid_fc_tokens': ['A', 'B'],                     # Forced choice options returned by the LLM-as-a-Judge\n",
    "    'valid_rs_tokens': ['A', 'B', 'AB'],               # Response set options returned by the LLM-as-a-Judge\n",
    "    'ratings_per_item': '3',                           # Number of human ratings per item (for display only)\n",
    "    'display': 'TopicalChat\\nUnderstandable'           # Human readable name of the task for plots\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb39032-9e48-4701-87ec-4f5dfdf8e1ec",
   "metadata": {},
   "source": [
    "### 2a. In configs/tasks.py -> get_task_forced_choice_distribution(), add a mapping that converts your human ratings columns to rating distributions:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ee46b75-ab4a-4c74-83a3-df04de920f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_forced_choice_distribution(task_name, df):\n",
    "    if task_name == 'my_task':\n",
    "        # Convert your specific rating columns to probabilities\n",
    "        # Example for binary Yes/No task:\n",
    "        df['Yes'] = df['n_positive'] / df['total_raters']  \n",
    "        df['No'] = df['n_negative'] / df['total_raters']\n",
    "        return df[['Yes', 'No']]\n",
    "    # ... existing code ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c51662f-05e3-435a-b5da-46afc60d14fe",
   "metadata": {},
   "source": [
    "### 2b. In configs/tasks.py -> get_task_reverse_fc_translation(), configure how forced-choice responses map back to response sets for your task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86b3a957-bee4-4577-9f02-dc58865ddba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_reverse_fc_translation(task_name, n_items, n_options, n_response_sets, beta=1):\n",
    "    \"\"\"\n",
    "    Defines the forced-choice translation matrix F' that maps forced-choice \n",
    "    responses back to response set distributions. This captures how raters\n",
    "    resolve rating indeterminacy when forced to select a single option.\n",
    "    \"\"\"\n",
    "    F_prime_template = np.zeros((n_options + 1, n_response_sets + 1))  \n",
    "    # +1 for invalid response handling. \n",
    "    # LLM refusals and invalid outputs get assigned this \"null\" option or response set \n",
    "    \n",
    "    if task_name == 'my_task':\n",
    "        # For binary task with options: Yes(0), No(1)\n",
    "        # Response sets: {Yes}(0), {No}(1), {Yes,No}(2), Invalid(3)\n",
    "        \n",
    "        F_prime_template[0, 0] = 1         # Yes -> definitely came from {Yes}\n",
    "        F_prime_template[1, 1] = 1-beta    # No -> came from {No} \n",
    "        F_prime_template[1, 0] = beta      # No -> might have come from {Yes} (indeterminacy)\n",
    "        F_prime_template[2, 3] = 1         # Invalid responses\n",
    "        \n",
    "    return np.tile(F_prime_template, (n_items, 1, 1))\n",
    "\n",
    "# The beta parameter is crucial:\n",
    "# - It represents P(positive ∈ response_set | forced_choice = negative)\n",
    "# - Set beta based on your task's expected indeterminacy level\n",
    "# - Can be estimated from paired FC/RS ratings if available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa887d44-f465-4674-a9da-0166f09ed3e5",
   "metadata": {},
   "source": [
    "### 3. Define your rating prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e29b13a-dcfa-4cda-8281-a593ca609f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPTS['my_task'] = {\n",
    "    'FC': \"\"\"Rate if {context} is {property}: {fact}\n",
    "    \n",
    "    Select ONE option that best applies:\n",
    "    A. Yes - The response is understandable\n",
    "    B. No - The response is not understandable\n",
    "    \n",
    "    RESPONSE FORMAT:\n",
    "    - Provide only a single letter: A or B\n",
    "    - No spaces, punctuation, or explanations\n",
    "    \n",
    "    Response: \"\"\",\n",
    "    \n",
    "    'RS': \"\"\"Rate if {context} is {property}: {fact}\n",
    "    \n",
    "    Select ALL options that could reasonably apply:\n",
    "    A. Yes - The response is understandable  \n",
    "    B. No - The response is not understandable\n",
    "    \n",
    "    RESPONSE FORMAT:\n",
    "    - Provide only the letter sequence (e.g., \"A\", \"B\", or \"AB\")\n",
    "    - No spaces, punctuation, or explanations\n",
    "    \n",
    "    Response: \"\"\"\n",
    "}\n",
    "# Note: Variables in {} will be replaced with values from columns specified in prompt_fields"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d29469-6785-42fc-964d-4d3b1021b3dd",
   "metadata": {},
   "source": [
    "## 4. Prepare your data file\n",
    "\n",
    "Save your data as a CSV in the `datasets/` directory with the following structure:\n",
    "\n",
    "### Required columns\n",
    "\n",
    "| Column Type | Description | Example |\n",
    "|------------|-------------|---------|\n",
    "| **Prompt fields** | Must match `prompt_fields` in config | `context`, `fact`, `response` |\n",
    "| **Rating columns** | Human annotation counts | `n_positive`, `n_negative`, `total_raters` |\n",
    "\n",
    "### Requirements\n",
    "- **Minimum 3 human ratings per item** for estimating forced choice distribution\n",
    "- **Column names must exactly match** those specified in `prompt_fields`\n",
    "- **Save location:** `datasets/your_task_name.csv`\n",
    "\n",
    "### Example CSV structure\n",
    "| context | fact | response | n_positive | n_negative | total_raters |\n",
    "|---------|------|----------|------------|------------|--------------|\n",
    "| User question | Model claim | Model response | 7 | 3 | 10 |\n",
    "| What's the weather? | It will rain tomorrow | Based on the forecast, yes | 8 | 2 | 10 |\n",
    "| How do I cook pasta? | Boil water first | Start by bringing a large pot of salted water to a boil | 9 | 1 | 10 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d856c29b-a135-46dd-adca-09abab91c8aa",
   "metadata": {},
   "source": [
    "# 5. Test your configuration:\n",
    "\n",
    "After completing steps 1-2b above, test your configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888cb1a6-d4ad-459e-885f-c8c92d35bbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Verify data loads correctly\n",
    "task_name = 'my_task'\n",
    "task_config = tasks.TASK_CONFIGS[task_name]\n",
    "corpus = loader.load_data(task_name, task_config, use_hf=False)\n",
    "print(f\"✓ Loaded {len(corpus)} items\")\n",
    "print(\"\\nFirst few items:\")\n",
    "print(corpus.head())\n",
    "\n",
    "# 5.2 Check prompt formatting works\n",
    "sample_item = corpus.iloc[0]\n",
    "fc_prompt = prompts.PROMPTS[task_name]['FC'].format(**{\n",
    "    field: sample_item[field] for field in task_config['prompt_fields']\n",
    "})\n",
    "print(\"\\n✓ Sample FC prompt (first 300 chars):\")\n",
    "print(fc_prompt[:300])\n",
    "\n",
    "# 5.3 Verify human rating distribution\n",
    "fc_dist = tasks.get_task_forced_choice_distribution(task_name, corpus)\n",
    "print(f\"\\n✓ Human rating distribution shape: {fc_dist.shape}\")\n",
    "print(\"Sample distribution for first item:\")\n",
    "print(fc_dist.iloc[0])\n",
    "\n",
    "# 5.4 Test with mock API calls for quick validation\n",
    "client = ModelAPIClient(\n",
    "    task_configs={task_name: task_config},\n",
    "    models=[models.MODELS[3]],  # Just test with one model\n",
    ")\n",
    "\n",
    "run_tag = f'test_{task_name}'\n",
    "client.run_tasks(run_tag=run_tag, mock=True)\n",
    "print(f\"\\n✓ Mock run completed for {task_name}\")\n",
    "\n",
    "# If all checks pass, your task is ready to use!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
